GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/glade/u/home/piyushag/.conda/envs/pa2/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /glade/work/piyushag/cmex_ml0/NN_IOdata_ckpts_Jul26onwards/tckpts exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-37504d2d-530a-b7cd-df68-7d417fe7be6c]
  | Name      | Type      | Params
----------------------------------------
0 | model     | MLP       | 73.2 K
1 | loss_func | HuberLoss | 0
----------------------------------------
73.2 K    Trainable params
0         Non-trainable params
73.2 K    Total params
0.293     Total estimated model params size (MB)
/glade/u/home/piyushag/.conda/envs/pa2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(










Epoch 0:  75%|â–‹| 2338/3118 [00:21<00:07, 110.53it/s, loss=0.143, v_num=zjb7, train_loss_step=0.144, train_RAE_step=185.0, train






































































































Epoch 1:  75%|â–‹| 2338/3118 [00:24<00:08, 96.52it/s, loss=0.138, v_num=zjb7, train_loss_step=0.140, train_RAE_step=183.0, train_




































































































Epoch 2:  75%|â–‹| 2338/3118 [00:22<00:07, 101.79it/s, loss=0.136, v_num=zjb7, train_loss_step=0.137, train_RAE_step=175.0, train




































































































Epoch 3:  75%|â–‹| 2338/3118 [00:23<00:07, 99.55it/s, loss=0.134, v_num=zjb7, train_loss_step=0.135, train_RAE_step=171.0, train_




































































































Epoch 4:  75%|â–‹| 2338/3118 [00:24<00:08, 96.78it/s, loss=0.133, v_num=zjb7, train_loss_step=0.132, train_RAE_step=173.0, train_




































































































Epoch 5:  75%|â–‹| 2338/3118 [00:23<00:07, 100.51it/s, loss=0.131, v_num=zjb7, train_loss_step=0.130, train_RAE_step=177.0, train






































































































Epoch 6:  75%|â–‹| 2338/3118 [00:23<00:07, 99.86it/s, loss=0.129, v_num=zjb7, train_loss_step=0.128, train_RAE_step=176.0, train_





































































































Epoch 7:  75%|â–‹| 2338/3118 [00:25<00:08, 92.21it/s, loss=0.128, v_num=zjb7, train_loss_step=0.127, train_RAE_step=166.0, train_



































































































Epoch 8:  75%|â–‹| 2338/3118 [00:23<00:07, 98.85it/s, loss=0.127, v_num=zjb7, train_loss_step=0.126, train_RAE_step=163.0, train_




































































































Epoch 9:  75%|â–‹| 2338/3118 [00:23<00:07, 98.64it/s, loss=0.126, v_num=zjb7, train_loss_step=0.125, train_RAE_step=157.0, train_




































































































Epoch 10:  75%|â–‹| 2338/3118 [00:22<00:07, 102.71it/s, loss=0.125, v_num=zjb7, train_loss_step=0.124, train_RAE_step=150.0, trai





































































































Epoch 11:  75%|â–‹| 2338/3118 [00:25<00:08, 91.81it/s, loss=0.123, v_num=zjb7, train_loss_step=0.122, train_RAE_step=149.0, train







































































































Epoch 12:  75%|â–‹| 2338/3118 [00:24<00:08, 94.62it/s, loss=0.122, v_num=zjb7, train_loss_step=0.121, train_RAE_step=151.0, train





































































































Epoch 13:  75%|â–‹| 2338/3118 [00:22<00:07, 104.23it/s, loss=0.121, v_num=zjb7, train_loss_step=0.120, train_RAE_step=154.0, trai




































































































Epoch 14:  75%|â–‹| 2338/3118 [00:22<00:07, 102.14it/s, loss=0.12, v_num=zjb7, train_loss_step=0.119, train_RAE_step=157.0, train





































































































Epoch 15:  75%|â–‹| 2338/3118 [00:22<00:07, 102.13it/s, loss=0.119, v_num=zjb7, train_loss_step=0.119, train_RAE_step=159.0, trai



































































































Epoch 16:  75%|â–‹| 2338/3118 [00:22<00:07, 102.47it/s, loss=0.118, v_num=zjb7, train_loss_step=0.118, train_RAE_step=159.0, trai







































































































Epoch 17:  75%|â–‹| 2338/3118 [00:22<00:07, 103.38it/s, loss=0.118, v_num=zjb7, train_loss_step=0.118, train_RAE_step=157.0, trai




































































































Epoch 18:  75%|â–‹| 2338/3118 [00:23<00:07, 97.92it/s, loss=0.118, v_num=zjb7, train_loss_step=0.117, train_RAE_step=155.0, train







































































































Epoch 19:  75%|â–‹| 2338/3118 [00:23<00:07, 97.71it/s, loss=0.117, v_num=zjb7, train_loss_step=0.117, train_RAE_step=154.0, train





































































































Epoch 20:  75%|â–‹| 2338/3118 [00:23<00:07, 99.93it/s, loss=0.117, v_num=zjb7, train_loss_step=0.116, train_RAE_step=154.0, train



























































































Validation DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 772/780 [03:00<00:01,  4.29it/s]

`Trainer.fit` stopped: `max_epochs=21` reached.
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-37504d2d-530a-b7cd-df68-7d417fe7be6c]














































































Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780/780 [02:35<00:00,  5.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m        Test metric        [22mâ”ƒ[1m       DataLoader 0        [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m      valid_CorrCoeff      [39mâ”‚[35m     0.999999946956581     [39mâ”‚
â”‚[36m         valid_MAE         [39mâ”‚[35m     298.9387041181814     [39mâ”‚
â”‚[36m        valid_MAE_0        [39mâ”‚[35m     2376.413393730821     [39mâ”‚
â”‚[36m        valid_MAE_1        [39mâ”‚[35m    0.01482918577894341    [39mâ”‚
â”‚[36m        valid_MAE_2        [39mâ”‚[35m    0.06209407999558588    [39mâ”‚
â”‚[36m        valid_MAE_3        [39mâ”‚[35m   0.054676566435282896    [39mâ”‚
â”‚[36m        valid_MAE_4        [39mâ”‚[35m    0.01536922085506294    [39mâ”‚
â”‚[36m        valid_MAE_5        [39mâ”‚[35m    0.07764314634369195    [39mâ”‚
â”‚[36m        valid_MAE_6        [39mâ”‚[35m    5.2349423694633845     [39mâ”‚
â”‚[36m        valid_MAE_7        [39mâ”‚[35m     9.636684645757384     [39mâ”‚
â”‚[36m         valid_RAE         [39mâ”‚[35m    348.41810410166477     [39mâ”‚
â”‚[36m        valid_RAE_0        [39mâ”‚[35m     3.588382852007913     [39mâ”‚
â”‚[36m        valid_RAE_1        [39mâ”‚[35m    44.439595782530645     [39mâ”‚
â”‚[36m        valid_RAE_2        [39mâ”‚[35m     2.090468967839388     [39mâ”‚
â”‚[36m        valid_RAE_3        [39mâ”‚[35m    12.668816342099843     [39mâ”‚
â”‚[36m        valid_RAE_4        [39mâ”‚[35m     23.23042358106231     [39mâ”‚
â”‚[36m        valid_RAE_5        [39mâ”‚[35m     715.7595684089208     [39mâ”‚
â”‚[36m        valid_RAE_6        [39mâ”‚[35m     565.4048145195065     [39mâ”‚
â”‚[36m        valid_RAE_7        [39mâ”‚[35m     1420.162762359348     [39mâ”‚
â”‚[36m        valid_loss         [39mâ”‚[35m    0.11717190037353944    [39mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜